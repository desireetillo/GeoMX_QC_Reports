---
author: "CPTR/CCR Genomics Core/Genome Analysis Unit"
title: "GeoMX DSP Transcriptome Atlas Initial QC"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '5'
    code_folding: hide
params:
  projectname: "demodata"
  output_prefix: "demodata"
  datadir: "demodata"
  DCCdir: "dccs"
  PKCFilename: "TAP_H_WTA_v1.0.pkc"
  WorkSheet: "kidney_demo_AOI_Annotations.xlsx"
  RemoveFlagSegment: FALSE
---


```{r setupandlibs, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(NanoStringNCTools)
library(GeomxTools)

datadir <- params$datadir
DCCdir <- params$DCCdir
PKCfilename<- params$PKCFilename
WorkSheet<- params$WorkSheet
output_prefix<-params$output_prefix
projectname<-params$projectname
RemoveFlagSegment<-params$RemoveFlagSegment
output_dir<-"processed_data/"

dir.create(output_dir)

```


# Introduction

This report outlines the initial QC for **`r projectname`** using the module **`r gsub(".pkc", "", PKCfilename)`**.  

For more information about this workflow, please see the [GeoMXWorkflows vignette](https://bioconductor.org/packages/release/workflows/vignettes/GeoMxWorkflows/inst/doc/GeomxTools_RNA-NGS_Analysis.html) on Bioconductor.



```{r loaddata, message = FALSE, eval = TRUE}


DCCFiles <- list.files(file.path(datadir , DCCdir), pattern=".dcc$", full.names=TRUE)
PKCFiles <- file.path(datadir, PKCfilename)
SampleAnnotationFile <- file.path(datadir, WorkSheet)

myData<-readNanoStringGeoMxSet(dccFiles = DCCFiles,
                                 pkcFiles = PKCFiles,
                                 phenoDataFile = SampleAnnotationFile,
                                 phenoDataSheet = "Template",
                                 phenoDataDccColName = "Sample_ID",
                                 protocolDataColNames = c("aoi", 
                                                          "roi"),
                                 experimentDataColNames = c("panel")) 

#Shift counts to one to mimic how DSPDA handles zero counts
myData <- shiftCountsOne(myData, elt="exprs", useDALogic=TRUE) 
pkcs <- annotation(myData)
modules <- gsub(".pkc", "", pkcs)
```


#  Segment QC


Before excluding any low-performing ROI/AOI segments, we visualize the distributions of the data for the different QC parameters.  The cutoffs used are:

* **Raw sequencing reads**: segments with <1000 raw reads are removed.
* **% Aligned,% Trimmed, or % Stitched sequencing reads**: segments below ~80% for one or more of these QC parameters are removed.
* **% Sequencing saturation ([1-deduplicated reads/aligned reads]%)**: segments below ~50% require additional sequencing to capture full sample diversity and are not typically analyzed until improved.
* **Negative Count**: this is the geometric mean of the several unique negative probes in the GeoMx panel that do not target mRNA and establish the background count level per segment; segments with low negative counts (1-10) are not necessarily removed but may be studied closer for low endogenous gene signal and/or insufficient tissue sampling.
* **No Template Control (NTC) count**: values >1,000 could indicate contamination for the segments associated with this NTC; however, in cases where the NTC count is between 1,000- 10,000, the segments may be used if the NTC data is uniformly low (e.g. 0-2 counts for all probes).
* **Nuclei**: >100 nuclei per segment is generally recommended; however, this cutoff is highly study/tissue dependent and may need to be reduced; what is most important is consistency in the nuclei distribution for segments within the study.  We use 20 in this case.
* **Area**: generally correlates with nuclei; a strict cutoff is not generally applied based on area.

_***Note***:  You may need to change these cutoffs depending on your experimental setup._


```{r setqcflagupdated,  eval = TRUE}
# Default QC cutoffs are commented in () adjacent to the respective parameters
# study-specific values were selected after visualizing the QC results in more
# detail below
QC_params <-
  list(minSegmentReads = 1000, # Minimum number of reads (1000)
       percentTrimmed = 80,    # Minimum % of reads trimmed (80%)
       percentStitched = 80,   # Minimum % of reads stitched (80%)
       percentAligned = 80,    # Minimum % of reads aligned to known targets (80%)
       percentSaturation = 50, # Minimum sequencing saturation (50%)
       minNegativeCount = 1,   # Minimum negative control counts (10)
       maxNTCCount = 1000,     # Maximum counts observed in NTC well (1000)
       minNuclei = 20,         # Minimum # of cells observed in a segment (100)
       minArea = 1000)         # Minimum segment area (5000)
myData <-
  setSegmentQCFlags(myData, 
                    qcCutoffs = QC_params)        

# Collate QC Results
QCResults <- protocolData(myData)[["QCFlags"]]
flag_columns <- colnames(QCResults)
QC_Summary <- data.frame(Pass = colSums(!QCResults[, flag_columns]),
                         Warning = colSums(QCResults[, flag_columns]))
QCResults$QCStatus <- apply(QCResults, 1L, function(x) {
  ifelse(sum(x) == 0L, "PASS", "WARNING")
})
QC_Summary["TOTAL FLAGS", ] <-
  c(sum(QCResults[, "QCStatus"] == "PASS"),
    sum(QCResults[, "QCStatus"] == "WARNING"))
```

## QC histograms {.tabset .tabset-fade}

### % Trimmed

``` {r plotQCHistograms Trimmed, eval = TRUE, warning = FALSE, message = FALSE}
library(ggplot2)

col_by <- "class"

# Graphical summaries of QC statistics plot function
QC_histogram <- function(assay_data = NULL,
                         annotation = NULL,
                         fill_by = NULL,
                         thr = NULL,
                         xlims = NULL) {
  if(is.null(xlims)) {
    xlims <- range(assay_data[[annotation]])
  }
  plt <- ggplot(assay_data,
                aes_string(x = paste0("unlist(`", annotation, "`)"),
                           fill = fill_by)) +
    geom_histogram(bins = 50) +
    geom_vline(xintercept = thr, lty = "dashed", color = "black") +
    theme_bw() + guides(fill = "none") +
    facet_wrap(as.formula(paste("~", fill_by)), nrow = 4) +
    xlim(xlims) +
    labs(x = annotation, y = "Segments, #", title = annotation)
  plt
}

QC_histogram(sData(myData), "Trimmed (%)", col_by, 80, c(0,101))
```

### % Stitched 
``` {r plotQCHistograms Stitched, eval = TRUE, warning = FALSE, message = FALSE}
QC_histogram(sData(myData), "Stitched (%)", col_by, 80, c(0,101))
```

### % Aligned 

``` {r plotQCHistograms Aligned, eval = TRUE, warning = FALSE, message = FALSE}
QC_histogram(sData(myData), "Aligned (%)", col_by, 80, c(0,101))
```

### % Saturated

``` {r plotQCHistograms Saturated, eval = TRUE, warning = FALSE, message = FALSE}

QC_histogram(sData(myData), "Saturated (%)", col_by, 50, c(0,101)) +
  labs(title = "Sequencing Saturation (%)",
       x = "Sequencing Saturation (%)")
```

### Area
``` {r plotQCHistograms area, eval = TRUE, warning = FALSE, message = FALSE}

QC_histogram(sData(myData), "area", col_by, 1000) +
  scale_x_continuous(trans = "log10")

```

### Nuclei count
``` {r plotQCHistograms nuclei, eval = TRUE, warning = FALSE, message = FALSE}

QC_histogram(sData(myData), "nuclei", col_by, 20)

```

### Negative Count

``` {r plotQCHistograms negGeoMean, eval = TRUE, warning = FALSE, message = FALSE}

# calculate the negative geometric means for each module
negativeGeoMeans <- 
  esBy(negativeControlSubset(myData), 
       GROUP = "Module", 
       FUN = function(x) { 
         assayDataApply(x, MARGIN = 2, FUN = ngeoMean, elt = "exprs") 
       }) 
protocolData(myData)[["NegGeoMean"]] <- negativeGeoMeans

# explicitly copy the Negative geoMeans from sData to pData
negCols <- paste0("NegGeoMean_", modules)
pData(myData)[, negCols] <- sData(myData)[["NegGeoMean"]]
for(ann in negCols) {
  plt <- QC_histogram(pData(myData), ann, col_by, 2) +
    scale_x_continuous(trans = "log10") 
  print(plt)
}

# detatch neg_geomean columns ahead of aggregateCounts call
pData(myData) <- pData(myData)[, !colnames(pData(myData)) %in% negCols]
```

## {.unlisted .unnumbered}


Below is a table summarizing the # of segments with a given NTC count:
```{r NTCcounts, results= "asis"}
library(knitr)

# show all NTC values, Freq = # of Segments with a given NTC count (for WTA samples this is the deduplicated read count from the empty well A01):
kable(table(NTC_Count = sData(myData)$NTC),
      col.names = c("NTC Count", "# of Segments"), caption = "NTC count summary")

if(length(sData(myData)$NTC >1000)){
  cat("Readout groups with high NTC counts:\n")
  unique(substr(rownames(sData(myData)),1,17)[sData(myData)$NTC>1000])
}
```


```{r QCSummaryTable, results = "asis"}
kable(QC_Summary, caption = "QC Summary Table for each Segment")
```

We are running this workflow with FlaggedQCRemoval set to `r RemoveFlagSegment`.  If this is TRUE we will now remove ROIs that did not pass our QC thresholds.

```{r RemoveFlagged, result="asis"}

if(RemoveFlagSegment){
  cat("We are now removing flagged segments\n")
  myData <- myData[, QCResults$QCStatus == "PASS"]
}
if(!RemoveFlagSegment){
  cat("We are NOT removing flagged segments\n")
}
dim(myData)


segment_qc_data<- data.frame(sData(myData))
segment_qc_data<-tibble::rownames_to_column(segment_qc_data,"DCCFileName")

segment_qc_data<-do.call(data.frame,segment_qc_data)
write.table(segment_qc_data,file=paste0(output_dir, output_prefix,'.SegmentQCMetrics.csv'), sep=',',row.names = F,col.names = T)

```

# Probe QC

Before we summarize our data into gene-level count data, we will remove low-performing probes. In short, this QC is an outlier removal process, whereby probes are either removed entirely from the study (global) or from specific segments (local). The QC applies to gene targets for which there are multiple distinct probes representing the count for a gene per segment. In WTA data, only one probe exists per target gene; thus, Probe QC does not apply to the endogenous genes in the panel. Rather, it is performed on the negative control probes; there are multiple probes representing our negative controls, which do not target any sequence in the genome. These probes enable calculation of the background per segment and will be important for determining gene detection downstream.

After Probe QC, there will always remain at least one probe representing every gene target. In other words, Probe QC never removes genes from your data.

### Set probe QC flags

A probe is removed globally from the dataset if either of the following is true:

* the geometric mean of that probe's counts from all segments divided by the geometric mean of all probe counts representing the target from all segments is less than 0.1
* the probe is an outlier according to the Grubb's test in at least 20% of the segments

A probe is removed locally (from a given segment) if the probe is an outlier according to the Grubb's test in that segment.

_***Note***:  We do not typically adjust these QC parameters._

```{r setbioprobeqcflag,  eval = TRUE}
# Generally keep the qcCutoffs parameters unchanged. Set removeLocalOutliers to 
# FALSE if you do not want to remove local outliers
myData <- setBioProbeQCFlags(myData, 
                               qcCutoffs = list(minProbeRatio = 0.1,
                                              percentFailGrubbs = 20), 
                               removeLocalOutliers = TRUE)

ProbeQCResults <- fData(myData)[["QCFlags"]]

# Define QC table for Probe QC
qc_df <- data.frame(Passed = sum(rowSums(ProbeQCResults[, -1]) == 0),
                    Global = sum(ProbeQCResults$GlobalGrubbsOutlier),
                    Local = sum(rowSums(ProbeQCResults[, -2:-1]) > 0
                                & !ProbeQCResults$GlobalGrubbsOutlier))
```

We report the number of global and local outlier probes and exclude those that do not pass Ratio and Global testing.

```{r bioprobeQCTable, echo = FALSE, results = "asis"}
kable(qc_df, caption = "Probes flagged or passed as outliers")
```

```{r excludeOutlierProbes, result="asis"}  

#Subset object to exclude all that did not pass Ratio & Global testing
cat("Before excluding probes:\n")
dim(myData)

ProbeQCPassed <- 
  subset(myData, 
         fData(myData)[["QCFlags"]][,c("LowProbeRatio")] == FALSE &
           fData(myData)[["QCFlags"]][,c("GlobalGrubbsOutlier")] == FALSE)

cat("\nAfter excluding probes:\n")

myData <- ProbeQCPassed 
dim(myData)
```

## Create gene-level count data

With our Probe QC steps complete, we will generate a gene-level count matrix. The count for any gene with multiple probes per segment is calculated as the geometric mean of those probes. The number below is the number of genes in the raw counts table.

```{r aggregateCounts, eval = TRUE}
# Check how many unique targets the object has
length(unique(featureData(myData)[["TargetName"]]))

# collapse to targets
target_myData <- aggregateCounts(myData)
raw_counts<-data.frame(exprs(target_myData))
raw_counts <- tibble::rownames_to_column(raw_counts, "Gene")
out_raw_counts<-paste0(output_dir, output_prefix,'_raw_counts.csv')
write.table(raw_counts,out_raw_counts,sep=",",row.names = F,col.names=T,quote=F)
```


# Segment and gene filtering 

In addition to Segment and Probe QC, we also determine the limit of quantification (LOQ) per segment. The LOQ is calculated based on the distribution of negative control probes and is intended to approximate the quantifiable limit of gene expression per segment. Please note that this process is more stable in larger segments. Likewise, the LOQ may not be as accurately reflective of true signal detection rates in segments with low negative probes counts (ex: <2). The formula for calculating the LOQ in a $i^{th}$ segment is: 

$$LOQ_{i} = geomean(NegProbe_{i}) * geoSD(NegProbe_{i})^{n}$$

Nanostring typically uses 2 geometric standard deviations ($n = 2$) above the geometric mean as the LOQ, which is reasonable for most studies, and recommend that a minimum LOQ of 2 be used if the LOQ calculated in a segment is below this threshold.

After determining the limit of quantification (LOQ) per segment, we recommend filtering out either segments and/or genes with abnormally low signal (below the LOQ). Filtering is an important step to focus on the true biological data of interest.



``` {r calculateLOQ, eval = TRUE}
# Define LOQ SD threshold and minimum value
cutoff <- 2
minLOQ <- 2

# Calculate LOQ per module tested
LOQ <- data.frame(row.names = colnames(target_myData))
for(module in modules) {
  vars <- paste0(c("NegGeoMean_", "NegGeoSD_"),
                 module)
  if(all(vars[1:2] %in% colnames(pData(target_myData)))) {
    LOQ[, module] <-
      pmax(minLOQ,
           pData(target_myData)[, vars[1]] * 
             pData(target_myData)[, vars[2]] ^ cutoff)
  }
}
pData(target_myData)$LOQ <- LOQ

LOQ_Mat <- c()
for(module in modules) {
  ind <- fData(target_myData)$Module == module
  Mat_i <- t(esApply(target_myData[ind, ], MARGIN = 1,
                   FUN = function(x) {
                     x > LOQ[, module]
                   }))
  LOQ_Mat <- rbind(LOQ_Mat, Mat_i)
}
# ensure ordering since this is stored outside of the geomxSet
LOQ_Mat <- LOQ_Mat[fData(target_myData)$TargetName, ]
```

## Segment filtering

We first filter out segments with exceptionally low signal. These segments will have a small fraction of panel genes detected above the LOQ relative to the other segments in the study. 

```{r segDetectionBarplotAndTable}

# Save detection rate information to pheno data
pData(target_myData)$GenesDetected <- 
  colSums(LOQ_Mat, na.rm = TRUE)
pData(target_myData)$GeneDetectionRate <-
  pData(target_myData)$GenesDetected / nrow(target_myData)


# Determine detection thresholds: 1%, 5%, 10%, 15%, >15%
pData(target_myData)$DetectionThreshold <- 
  cut(pData(target_myData)$GeneDetectionRate,
      breaks = c(-1, 0.01, 0.05, 0.1, 0.15, 1),
      labels = c("<1%", "1-5%", "5-10%", "10-15%", ">15%"))

# stacked barplot of different cutpoints (1%, 5%, 10%, 15%)
ggplot(pData(target_myData),
       aes(x = DetectionThreshold)) +
  geom_bar(aes(fill = `class`)) +
  geom_text(stat = "count", aes(label = after_stat(count)), vjust = -0.5) +
  theme_bw() +
  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +
  labs(x = "Gene Detection Rate",
       y = "Segments, #",
       fill = "Segment class")


library(DT)
tmp_data<-pData(target_myData)
tmp_data<-tmp_data[,c("class","region","GenesDetected","GeneDetectionRate")]
datatable(tmp_data,
          'Table showing the genes detected for each ROI',
          options=list(pageLength = 10)
) %>%  formatRound(c(4), 2)

```

Table showing the tissue classes impacted by each threshold:

```{r segTable}
# cut percent genes detected at 1, 5, 10, 15
kable(table(pData(target_myData)$DetectionThreshold,
            pData(target_myData)$class))
```


Generally, 5-10% detection is a reasonable segment filtering threshold. For now, we will select segments that have a 5% detection rate. 

_***Note***: These guidelines may require adjustment depending on the experimental design  (e.g. segment types, size, nuclei) and tissue characteristics (e.g. type, age)._

```{r filterSegments}
geneDetectionRateThresh<-0.05

target_myData <-
  target_myData[, pData(target_myData)$GeneDetectionRate >= geneDetectionRateThresh]

dim(target_myData)
```


``` {r goi detection}

# Calculate detection rate:
LOQ_Mat <- LOQ_Mat[, colnames(target_myData)]
fData(target_myData)$DetectedSegments <- rowSums(LOQ_Mat, na.rm = TRUE)
fData(target_myData)$DetectionRate <-
  fData(target_myData)$DetectedSegments / nrow(pData(target_myData))

```

## Gene filtering

We will graph the total number of genes detected in different percentages of segments. Based on the visualization below, we can better understand global gene detection in our study and select how many low detected genes to filter out of the dataset. Gene filtering increases performance of downstream statistical tests and improves interpretation of true biological signal.

```{r plotDetectionRate, eval = TRUE}
# Plot detection rate:
plot_detect <- data.frame(Freq = c(1, 5, 10, 20, 30, 50))
plot_detect$Number <-
  unlist(lapply(c(0.01, 0.05, 0.1, 0.2, 0.3, 0.5),
                function(x) {sum(fData(target_myData)$DetectionRate >= x)}))
plot_detect$Rate <- plot_detect$Number / nrow(fData(target_myData))
rownames(plot_detect) <- plot_detect$Freq

ggplot(plot_detect, aes(x = as.factor(Freq), y = Rate, fill = Rate)) +
    geom_bar(stat = "identity") +
    geom_text(aes(label = formatC(Number, format = "d", big.mark = ",")),
              vjust = 1.6, color = "black", size = 4) +
    scale_fill_gradient2(low = "orange2", mid = "lightblue",
                         high = "dodgerblue3", midpoint = 0.65,
                         limits = c(0,1),
                         labels = scales::percent) +
    theme_bw() +
    scale_y_continuous(labels = scales::percent, limits = c(0,1),
                       expand = expansion(mult = c(0, 0))) +
    labs(x = "% of Segments",
         y = "Genes Detected, % of Panel > LOQ")
```


We will now select genes detected in at least 10% of segments and filter out the remainder of the targets. 

_***Note:*** if we know that a key gene is represented in only a small number of segments (<10%) due to biological diversity, we may select a different cutoff or keep the target gene by manually selecting them for inclusion in the data object._



``` {r subsetGenes, eval = TRUE}
# Subset to target genes detected in at least 1 of the samples.
#   Also manually include the negative control probe, for downstream use
targetDetectionRateThresh<-0.10
negativeProbefData <- subset(fData(target_myData), CodeClass == "Negative")
neg_probes <- unique(negativeProbefData$TargetName)
target_myData <- 
   target_myData[fData(target_myData)$DetectionRate > targetDetectionRateThresh |
                     fData(target_myData)$TargetName %in% neg_probes, ]

"Remaining targets and ROIs"
dim(target_myData)


```


# Normalization

We will now normalize the GeoMx data for downstream visualizations and differential expression. The two common methods for normalization of DSP-NGS RNA data are i) quartile 3 (Q3) or ii) background normalization.

Q3 normalization is typically the preferred normalization strategy for most DSP-NGS RNA studies, so we will perform Q3 normalization here.

Before normalization, we will explore the relationship between the upper quartile (Q3) of the counts in each segment with the geometric mean of the negative control probes in the data. Ideally, there should be a separation between these two values to ensure we have stable measure of Q3 signal. If you do not see sufficient separation between these values, you may consider more aggressive filtering of low signal segments/genes.   For additional conceptual guidance, please refer to Nanostring's [Data Analysis White Paper for DSP-NGS Assays](https://www.nanostring.com/resources/geomx-cta-data-whitepaper/). 


``` {r, previewNF, fig.width = 8, fig.height = 8, fig.wide = TRUE, eval = TRUE, warning = FALSE, message = FALSE}
library(reshape2)  # for melt
library(cowplot)   # for plot_grid

# Graph Q3 value vs negGeoMean of Negatives
ann_of_interest <- "class"
Stat_data <- 
  data.frame(row.names = colnames(exprs(target_myData)),
             Segment = colnames(exprs(target_myData)),
             Annotation = pData(target_myData)[, ann_of_interest],
             Q3 = unlist(apply(exprs(target_myData), 2,
                               quantile, 0.75, na.rm = TRUE)),
             NegProbe = exprs(target_myData)[neg_probes, ])
Stat_data_m <- melt(Stat_data, measure.vars = c("Q3", "NegProbe"),
                    variable.name = "Statistic", value.name = "Value")

plt1 <- ggplot(Stat_data_m,
               aes(x = Value, fill = Statistic)) +
  geom_histogram(bins = 40) + theme_bw() +
  scale_x_continuous(trans = "log2") +
  facet_wrap(~Annotation, ncol = 3) + 
  scale_fill_brewer(palette = 3, type = "qual") +
  labs(x = "Counts", y = "Segments, #")

plt2 <- ggplot(Stat_data,
               aes(x = NegProbe, y = Q3, color = Annotation)) +
  geom_abline(intercept = 0, slope = 1, lty = "dashed", color = "darkgray") +
  geom_point() + guides(color = "none") + theme_bw() +
  scale_x_continuous(trans = "log2") + 
  scale_y_continuous(trans = "log2") +
  theme(aspect.ratio = 1) +
  labs(x = "Negative Probe GeoMean, Counts", y = "Q3 Value, Counts")

plt3 <- ggplot(Stat_data,
               aes(x = NegProbe, y = Q3 / NegProbe, color = Annotation)) +
  geom_hline(yintercept = 1, lty = "dashed", color = "darkgray") +
  geom_point() + theme_bw() +
  scale_x_continuous(trans = "log2") + 
  scale_y_continuous(trans = "log2") +
  theme(aspect.ratio = 1) +
  labs(x = "Negative Probe GeoMean, Counts", y = "Q3/NegProbe Value, Counts")

btm_row <- plot_grid(plt2, plt3, nrow = 1, labels = c("B", ""),
                     rel_widths = c(0.43,0.57))
plot_grid(plt1, btm_row, ncol = 1, labels = c("A", ""))
```



```{r normalizeObject, eval = TRUE}
# Q3 norm (75th percentile) for WTA/CTA  with or without custom spike-ins
target_myData <- normalize(target_myData , 
                             norm_method = "quant", 
                             desiredQuantile = .75,
                             toElt = "q_norm")

## save q3 data as csv file

norm_data<-data.frame(assayDataElement(target_myData , elt = "q_norm"))
norm_data <- tibble::rownames_to_column(norm_data, "Gene")
out_norm_data<-paste0(output_dir, output_prefix,'_Q3_normdata.',
                      'geneDetect',
                      geneDetectionRateThresh,
                      '_',
                      'targetDetect',
                      targetDetectionRateThresh,
                      '.csv')

write.table(norm_data,out_norm_data,sep=",",row.names = F,col.names=T,quote=F)
# lets also save the entire targetmyData as an rstructure
save(target_myData, file = paste0(output_dir, output_prefix,".RData"))

```

To demonstrate the effects of normalization, we graph representative boxplots of the data for individual segments before and after normalization.

```{r normplot, fig.width=8}
# visualize the first 10 segments with each normalization method
boxplot(exprs(target_myData)[,1:20],
        col = "#9EDAE5", main = "Raw Counts",
        log = "y", names = 1:20, xlab = "Segment",
        ylab = "Counts, Raw",
        ylim=c(1,40000))

boxplot(assayDataElement(target_myData[,1:20], elt = "q_norm"),
        col = "#2CA02C", main = "Q3 Norm Counts",
        log = "y", names = 1:20, xlab = "Segment",
        ylab = "Counts, Q3 Normalized",
         ylim=c(1,15000))

```

# Unsupervised analysis

Now that we have filtered and normalized our data set we can explore the data. 

## UMAP,t-SNE, PCA, heatmap {.tabset .tabset-fade}

### UMAP
``` {r dimReduction UMAP, eval = TRUE,message=FALSE, warning=FALSE}
library(umap)

myshapes<-c(16,17,18,15,21,22,3,42,4,8)
# update defaults for umap to contain a stable random_state (seed)
custom_umap <- umap::umap.defaults
custom_umap$random_state <- 42
# run UMAP
umap_out <-
  umap(t(log2(assayDataElement(target_myData , elt = "q_norm"))),  
       config = custom_umap)
pData(target_myData)[, c("UMAP1", "UMAP2")] <- umap_out$layout[, c(1,2)]
ggplot(pData(target_myData),
       aes(x = UMAP1, y = UMAP2, color = class, shape=region)) +
  geom_point(size = 4) +
  scale_shape_manual(values=myshapes) +
  theme_bw()
```

### t-SNE

``` {r dimReduction tSNE, eval = TRUE,message=FALSE, warning=FALSE}
library(Rtsne)
# run tSNE
set.seed(42) # set the seed for tSNE as well
tsne_out <-
  Rtsne(t(log2(assayDataElement(target_myData , elt = "q_norm"))),
        perplexity = ncol(target_myData)*.15)
pData(target_myData)[, c("tSNE1", "tSNE2")] <- tsne_out$Y[, c(1,2)]
ggplot(pData(target_myData),
       aes(x = tSNE1, y = tSNE2, color = class, shape=region)) +
  geom_point(size = 4) +
  scale_shape_manual(values=myshapes) +
  theme_bw()
```


### PCA
``` {r dimReduction PCA, eval = TRUE,message=FALSE, warning=FALSE}
PCAx<-1
PCAy<-2
PCAz<-3

PCAxyz <- c(as.integer( PCAx ),as.integer( PCAy), as.integer(PCAz) ) # selected principal components


pca.object <- prcomp(t(log2(assayDataElement(target_myData , elt = "q_norm"))))
pcaData = as.data.frame(pca.object$x[, PCAxyz]); 
pData(target_myData)[, c("PC1", "PC2", "PC3")] <- pcaData[,c(1,2,3)]
percentVar=round(100*summary(pca.object)$importance[2, PCAxyz],0)


ggplot(pData(target_myData),
       aes(x = PC1, y = PC2, color = class, shape=region)) +
  geom_point(size = 4) +
  xlab(paste0("PC", PCAx ,": ", percentVar[1], "% variance")) +
  ylab(paste0("PC", PCAy ,": ", percentVar[2], "% variance")) +
  scale_shape_manual(values=myshapes) +
  theme_bw()
```

### Interactive PCA plot

_**Note**: this only works for up to 8 region types._

```{r InteractivePCA, eval = TRUE,message=FALSE, warning=FALSE,out.width="100%", out.height="100%"}

library(plotly)

m <- list(
    l = 5,
    r = 5,
    b = 0,
    t = 0,
    pad = 0
)
fig <- plot_ly(pData(target_myData),
               x = ~PC1, 
               y = ~PC2, 
               z = ~PC3, 
               color = ~class,
               symbol=~region, 
               marker=list(size=4),
               symbols = c('circle','diamond','cross',  'x', 'square','o','diamond-open','square-open'),
               colors='Dark2',
               width = 700, height = 700)

fig <- fig %>% add_markers()
fig <- fig %>% layout(scene = list(xaxis = list(title = paste0('PC1 (', percentVar[1],"%)" )),
                     yaxis = list(title = paste0('PC2 (', percentVar[2],"%)" )),
                     zaxis = list(title = paste0('PC3 (', percentVar[3],"%)" ))),
                     margin=m)

fig

```

### Heatmap

``` {r CVheatmap, eval = TRUE, echo = TRUE, fig.width = 8, fig.height = 6.5, fig.wide = TRUE}
library(pheatmap)  # for pheatmap
# create a log2 transform of the data for analysis
assayDataElement(object = target_myData, elt = "log_q") <-
  assayDataApply(target_myData, 2, FUN = log, base = 2, elt = "q_norm")

# create CV function
calc_CV <- function(x) {sd(x) / mean(x)}
CV_dat <- assayDataApply(target_myData,
                         elt = "log_q", MARGIN = 1, calc_CV)

# Identify genes in the top 3rd of the CV values
GOI <- names(CV_dat)[CV_dat > quantile(CV_dat, 0.8)]


pheatmap(assayDataElement(target_myData[GOI, ], elt = "log_q"),
         scale = "row", 
         main="Heatmap of high CV (>0.8) genes",
         show_rownames = FALSE, show_colnames = FALSE,
         border_color = NA,
         clustering_method = "average",
         clustering_distance_rows = "correlation",
         clustering_distance_cols = "correlation",
         breaks = seq(-3, 3, 0.05),
         color = colorRampPalette(c("purple3", "black", "yellow2"))(120),
         annotation_col = pData(target_myData)[, c("class", "region")])

```

## {.unlisted .unnumbered}

# R tool version information

```{r Info, echo=FALSE, message=FALSE, warning=FALSE}
sessionInfo()
```

